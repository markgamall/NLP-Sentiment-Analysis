{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries & Data (review_polarity)"
      ],
      "metadata": {
        "id": "RJ0IqFT92_Qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERXngm8yelMZ",
        "outputId": "82c964fd-cc60-46dc-b1d7-a79b1cc3909b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ7MnTV1h_bA",
        "outputId": "0a8e11ed-7c30-4214-c4f9-eba669d7b25c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm pandas nltk pyspellchecker emoji"
      ],
      "metadata": {
        "id": "q_AD_WL1e-xr",
        "outputId": "1697cb7d-b47c-45e7-9d03-55b3d58f583a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker, emoji\n",
            "Successfully installed emoji-2.14.1 pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import string\n",
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "import emoji\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report,accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.sparse import hstack\n",
        "import contractions\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "jmxTxTOVe3Ye"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/txt_sentoken'\n",
        "neg_path = os.path.join(base_path, 'neg')\n",
        "pos_path = os.path.join(base_path, 'pos')\n",
        "\n",
        "neg_reviews = []\n",
        "for file in os.listdir(neg_path):\n",
        "    with open(os.path.join(neg_path, file), 'r', encoding='utf-8') as f:\n",
        "        neg_reviews.append(f.read())\n",
        "\n",
        "pos_reviews = []\n",
        "for file in os.listdir(pos_path):\n",
        "    with open(os.path.join(pos_path, file), 'r', encoding='utf-8') as f:\n",
        "        pos_reviews.append(f.read())\n",
        "\n",
        "df_txt_sentoken = pd.DataFrame({\n",
        "    'text': neg_reviews + pos_reviews,\n",
        "    'label': [0]*len(neg_reviews) + [1]*len(pos_reviews)\n",
        "})\n",
        "\n",
        "print(df_txt_sentoken.head())\n",
        "print(df_txt_sentoken['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3D5igHqfPHN",
        "outputId": "8c49eab3-e111-4c9f-d496-6896a2c56b39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  label\n",
            "0  plot : two teen couples go to a church party ,...      0\n",
            "1  the happy bastard's quick movie review \\ndamn ...      0\n",
            "2  it is movies like these that make a jaded movi...      0\n",
            "3   \" quest for camelot \" is warner bros . ' firs...      0\n",
            "4  synopsis : a mentally unstable man undergoing ...      0\n",
            "label\n",
            "0    1000\n",
            "1    1000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "hyjR8hTDtUX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import contractions\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # You can use 'en_core_web_md' for better results\n",
        "\n",
        "# Sample slang dictionary (you can expand this)\n",
        "slang_dict = {\n",
        "    \"u\": \"you\", \"ur\": \"your\", \"lol\": \"laugh out loud\", \"idk\": \"i do not know\",\n",
        "    \"brb\": \"be right back\", \"btw\": \"by the way\", \"imo\": \"in my opinion\",\n",
        "    \"omg\": \"oh my god\", \"lmk\": \"let me know\", \"smh\": \"shaking my head\"\n",
        "}\n",
        "\n",
        "# Optional: Spelling correction using TextBlob or SymSpell (not included here due to slowness)\n",
        "# from textblob import TextBlob\n",
        "\n",
        "# Expand slang\n",
        "def expand_slang(text, slang_dict):\n",
        "    words = text.split()\n",
        "    return ' '.join([slang_dict.get(w, w) for w in words])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text, rare_words=set()):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Expand contractions (e.g., don't -> do not)\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    # Expand slang\n",
        "    text = expand_slang(text, slang_dict)\n",
        "\n",
        "    # Remove digits and punctuation\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Lemmatization with spaCy (better than NLTK)\n",
        "    doc = nlp(text)\n",
        "    lemmatized = []\n",
        "    for token in doc:\n",
        "        word = token.text\n",
        "        # OPTIONAL: Apply spelling correction only to rare words\n",
        "        if word in rare_words:\n",
        "            pass  # Add correction logic here if needed\n",
        "        lemmatized.append(token.lemma_)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned = [word for word in lemmatized if word not in stop_words]\n",
        "\n",
        "    return ' '.join(cleaned)\n",
        "\n",
        "\n",
        "# Compute word frequencies to identify rare words\n",
        "all_words = ' '.join(df_txt_sentoken['text'].str.lower()).split()\n",
        "word_freq = Counter(all_words)\n",
        "rare_words = {word for word, freq in word_freq.items() if freq == 1}\n",
        "\n",
        "# Apply the preprocessing\n",
        "df_txt_sentoken['clean_text'] = df_txt_sentoken['text'].apply(lambda x: preprocess_text(str(x), rare_words))\n",
        "\n",
        "final_df = df_txt_sentoken[['clean_text', 'label']]\n",
        "\n",
        "# View result\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_gpNguBiVxs",
        "outputId": "fe82819e-ed81-4603-d332-f9d5b14354a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text  label\n",
            "0  plot two teen couple go church party drink dri...      0\n",
            "1  happy bastard quick movie review damn yk bug g...      0\n",
            "2  movie like make jade movie viewer thankful inv...      0\n",
            "3  quest camelot warner bros first featurelength ...      0\n",
            "4  synopsis mentally unstable man undergo psychot...      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP7_RtongGrr",
        "outputId": "c92b58c0-fef4-4d47-b199-272d50ba6849"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "7uZ7Wm7u3OCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization (Unigrams + Bigrams)\n",
        "tfidf = TfidfVectorizer(min_df=5, max_features=10000, ngram_range=(1, 2))\n",
        "X_tfidf = tfidf.fit_transform(final_df['clean_text'])\n",
        "y = final_df['label']\n",
        "\n",
        "# Train/test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "A29wzPrpstQR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "TCZvsy7E3Qyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Logistic Regression\n",
        "# ==========================\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best Logistic Regression (GridSearchCV) Accuracy:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQn1WIv5sxv_",
        "outputId": "4563212e-6067-45d4-ceb1-843a821ef7a0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression (GridSearchCV) Accuracy: 0.8425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# LinearSVC\n",
        "# ==========================\n",
        "svc = LinearSVC()\n",
        "svc.fit(X_train, y_train)\n",
        "y_pred_svc = svc.predict(X_test)\n",
        "print(\"LinearSVC Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svc))\n",
        "\n",
        "# ==========================\n",
        "# Random Forest\n",
        "# ==========================\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TTL961Wyqm_",
        "outputId": "bd6b401e-0e61-4d39-c4da-8fbc1c62c46b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC Accuracy: 0.84\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.81      0.84       200\n",
            "           1       0.82      0.86      0.84       200\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n",
            "Random Forest Accuracy: 0.825\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.84      0.83       200\n",
            "           1       0.84      0.81      0.82       200\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.83      0.82      0.82       400\n",
            "weighted avg       0.83      0.82      0.82       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Ensemble Learning\n",
        "# ==========================\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('lr', LogisticRegression(max_iter=1000)),\n",
        "    ('nb', MultinomialNB()),\n",
        "    ('rf', RandomForestClassifier())\n",
        "], voting='soft')  # Use 'hard' or 'soft'\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "preds = ensemble.predict(X_test)\n",
        "print(\"Ensemble Accuracy:\", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3O8r8l9zQW5",
        "outputId": "f816077a-aef9-4645-e759-4c40ce6bae5a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 0.845\n"
          ]
        }
      ]
    }
  ]
}